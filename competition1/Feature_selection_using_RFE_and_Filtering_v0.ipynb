{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True  True]\n",
      "[2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# create a base classifier used to evaluate a subset of attributes\n",
    "model = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 3)\n",
    "rfe = rfe.fit(dataset.data, dataset.target)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results showing that the latter 3 features are selected in the model (`True` means this feature is selected). The second row shows the ranking of the features if you want to use the __filter__ method.\n",
    "\n",
    "Note that you do not need to check the performance (f1-score) or goodness-of-fit (AUC) in your model here since `RFE` automatically select the best subset of features.\n",
    "\n",
    "However, we abitrarily select __3 features__ from the original 4. This is not scientific. What you should do is that test different number of features in the subset. You can use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Checking how many features this dataset have\n",
    "print(len(dataset.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model with the best 1 features\n",
      "[False False False  True]\n",
      "[4 2 3 1]\n",
      "2\n",
      "Model with the best 2 features\n",
      "[False  True False  True]\n",
      "[3 1 2 1]\n",
      "3\n",
      "Model with the best 3 features\n",
      "[False  True  True  True]\n",
      "[2 1 1 1]\n",
      "4\n",
      "Model with the best 4 features\n",
      "[ True  True  True  True]\n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(dataset.data[0])+1):\n",
    "    print(i)\n",
    "    # create a base classifier used to evaluate a subset of attributes\n",
    "    model = LogisticRegression()\n",
    "    # create the RFE model and select 3 attributes\n",
    "    rfe = RFE(model, i)\n",
    "    rfe = rfe.fit(dataset.data, dataset.target)\n",
    "    # summarize the selection of the attributes\n",
    "    print('Model with the best', i, 'features')\n",
    "    print(rfe.support_)\n",
    "    print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you should have different subsets of selected into the __Evaluation code__ provided to check model performance.\n",
    "\n",
    "`RFE` is like a black box - you can use a different method which you can look into the feature selection process, and have more control.\n",
    "\n",
    "Decision tree based classifiers, such as random forest classifier or extra trees classifier, can report feature importance - which you can use as a threshold to filter features. \n",
    "\n",
    "See code below - in code below, we use extra trees classifier to report feature importance, and use an abitrary `0.4` value to filter the features. The idea is whatever features passing the threshold would be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_ > .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results show that the 3rd and 4th features passed our threshold, hence should be selected.\n",
    "\n",
    "Of course, abitrarily select the threshold at `.4` is not scientific. You should also try different values in a for loop.\n",
    "\n",
    "# References\n",
    "- [Kaggle - Step Forward Feature Selection: A Practical Example in Python](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html)\n",
    "- [MLM - Feature Selection in Python with Scikit-Learn](https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
